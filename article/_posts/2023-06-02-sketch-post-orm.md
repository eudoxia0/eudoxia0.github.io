---
title: Sketch of a Post-ORM
summary: A better way to interact with databases.
card: sketch-post-orm.jpg
card_source: |
    Screenshot of the [Connection Machine][cm] document retrieval system.

    [cm]: https://en.wikipedia.org/wiki/Connection_Machine
---

- intro
  - been doing database work
  - it's frustrating
  - surely we should have solved this
- state of database access
  - bimodal
    - case 1: use raw sql
      - pros:
        - can optimize queries endlessly
        - can use the power-user features of your specific rdbms
        - easy to know where queries are happening
          - can centralize query access in specific modules
            - which lets you optimize those modules separately, implement pre and post-save features
      - cons:
        - it looks like this [pic of java querying example]
        - type checking disappears at the SQL boundary
        - SQL has problems (more on this later)
    - case 2: use an orm
      - orm means object relational mapper
        - but let's not get caught up by syntax
        - by orm i mean
          - a library that takes over database interaction
          - where you can design "languag-first" rather than sql first
          - you write classes (or records, depending on the language youre using)
          - and sprinkle annotations maybe
          - and you get auto-generated schemas and sql
          - the thing manages creating and alternating schemas and making queries
          - typically the goal is to have a succint (if not simple) way to interact with the database
          - in other words, something is an orm if it lets you write code that looks like this: [example]
          - no oop needed
      - pros:
        - very quick to write
        - looks like this: [pic of a django orm code]
        - expedient
      - cons:
        - queries can be badly optimized
        - orms often emphasize portability, at the cost of specificity
        - "just optimize your hot loops lol"
          - let's be realistic
          - how many of you are measuring your query performance?
          - how many of you are actually going to be rewriting your queries to use pgsql prepared statements?
          - the path of least resistance is to use the orm exclusively, so you have the orm's performance bedrock
        - knowledge doesn't transfer
          - knowing how to optimize a django orm query doesn't transfer to optimizing a raw sql query
        - hard to go from one orm to another
        - query smearing
          - shotgun queries: database access is smeared across the application
          - easy to take a queryset, pass it to another function that then does something else with it (which in turn triggers a query)
          - very hard to statically determine where your database code is
          - this in turn makes it hard to do things like: whenever we update a model X, run this job or whatever.
            - "just use triggers" doesn't solve this because native rdbms features may not play well with the orm!
  - there is a missing middle
    - something more convenient than the jdbc example
    - has less problems than orms
- background
  - what license do i have to speak of this?
  - not much
  - most of my experience is using the django orm
  - in a past life i wrote my own orm inspired by the django orm
    - for common lisp
    - with automatic migrations using schema diffing
    - i started a rewrite that I never ended
  - not a dba
  - don't know enough about raw sql
  - but actively working on learning
- sketch
  - migration-first
    - "just write raw sql lol" isn't practical advice much of the time
    - doesn't solve the problem of migrations
    - you can either
      - use liquidbase
      - roll your own migration architecture
    - orms are schema first
      - write the schema
      - generate migrations by diffing against the last known good version of the schema
      - convenient
        - do what i mean
        - i give you the schema i want, you figure out how to get there
      - problem is it underemphasizes migrations
      - i'm not sure why, i dont think i can argue this rigorously, but i think migrations should come first
      - you write your migrations
        - either as code, or as a declarative format like json
        - but ideally not as SQL
        - the problem with writing raw SQL is it's very hard to bring it up to the level where it can be manipulated programmatically
        - you want to be able to handle migrations as first class objects, which means: parse them, compare them, serialize them, turn them to documentation
      - then a tool runs those migrations virtually, starting with an empty schema, applying one migration at a time, and dumps the resulting schema to a file where it can be visualized
      - also can generate schema docs
      - this is similar to how code-first graphql libraries let you define your graphql schema as code and them dump a schema.gql file that the frontend can pick up
  - unportable for databases
    - many orms and db access libraries advertise portability as a feature
    - in reality: sql is never portable
      - sqlite vs. everything else is completely differnet universes
      - sqlite aside, within big iron (pg, mysql, microsoft, oracle) the differences are huge
        - sql has different syntax
        - different types
        - differnet features
        - different perf characteristics
      - the choice of postgres vs. mysql is not like ext4 vs zfs.
        - with that choice, there exists a semantic level above which the differences disappear. dd and du work the same.
        - it's more like choosing between Python and C.
    - abandon the whole concept of database portability
      - just use postgres
      - shamelessly exploit native features
      - switching from one db to another rarely happens
  - portable across languages
    - like openapi spec
    - your migrations are json
    - your queries are written in some separate language
    - code generator creates bindings to whatever language you're using to
      - apply the migrations
      - run those queries with type checking
  - post-sql
    - sql is bad
      - syntax is highly irregular
        - hard to parse
        - hard to learn
        - hard to remember
        - select statement grammar has like 7 holes for subexpressions
        - you can argue back and forth about whether this is or isn't good or beginner-friendly
        - i'm explicitly ignoring the point of view of beginners programmers or business analysts
        - my point of view is: programmers who want to access a database
        - they want type checking and they want a sane, manipulable, decently functional syntax
        - they want neither anti-intellectal just git er done dynamic typing tarpits or trans-dimensional monad optic stacks
      - type checking is absent
        - matters less if youre using the database interactiely, like a business analyst
        - again, irrelevant: as long as the underlying database is SQL, you can use SQL if you want
        - but
        - from the point of view of an _application_, your queries are fixed, it's just the parameters that have different values (but usually fixed types)
      - i want a query language with
        - sane syntax
        - type checking
        - compiles to Postgres SQL
      - this is easier said than done
      - the challenge has three components
        - design a sane query language with sane syntax and type checking
        - make it feature-complete to native postgres
        - make it compile to efficient sql
  - discriminated records
    - we need sum types
    - java has sum types now
    - the world needs sum types
    - humanity cannot survive this century without sum types
  - maybe we can use stored procedures
- the complete sketch
